\documentclass{article}

\setlength{\topmargin}{-.5in}
\setlength{\oddsidemargin}{.125in}
\setlength{\textwidth}{6.25in}

% page numbering style
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\fancyfoot[R]{\thepage}
\fancypagestyle{plain}{%
    \renewcommand{\headrulewidth}{0pt}%
    \fancyhf{}%
    \fancyfoot[R]{\thepage}%
}

\usepackage{float}

\begin{document}
\SweaveOpts{concordance=TRUE, echo=TRUE}

<<echo=FALSE>>=
library(ggplot2)
library(rstan)
library(ggcorrplot)
library(reshape2)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
rstan_ggtheme_options(legend.position = 'none')

set.seed(0)
@

<<echo=FALSE>>=
data.cleaned = read.csv('../data/stack_overflow_survey_2016_cleaned.csv')
@

<<echo=FALSE>>=
# helper functions
CountNonNa = function(x) {length(x[!is.na(x)])}
min.occurences = 35

GetNonNaDataCountry = function(summary.col, additional.cols = NULL) {
  counts = tapply(data.cleaned[, summary.col], data.cleaned$country, CountNonNa)
  countries = names(counts[counts >= min.occurences])

  result.cols = c('country', summary.col, additional.cols)
  # todo: impute missing values
  df.selected = na.omit(data.cleaned[data.cleaned$country %in% countries, result.cols])
  df.other = na.omit(data.cleaned[!data.cleaned$country %in% countries, result.cols])
  df.other$country = 'Other'

  return(rbind(df.selected, df.other))
}

GetNonNaDataIndustry = function(summary.col, additional.cols = NULL) {
  counts = tapply(data.cleaned[, summary.col], data.cleaned$industry, CountNonNa)
  industries = names(counts[counts >= min.occurences])

  result.cols = c('industry', summary.col, additional.cols)
  # todo: impute missing values
  df.selected = na.omit(data.cleaned[data.cleaned$industry %in% industries, result.cols])
  df.other = na.omit(data.cleaned[!data.cleaned$industry %in% industries, result.cols])
  df.other$industry = 'Other'

  return(rbind(df.selected, df.other))
}

DummyEncode = function(df, col.name) {
  dummy = model.matrix(~factor(df[, col.name]))
  colnames(dummy) = paste(col.name, 1:ncol(dummy), sep = '_')
  df = cbind(df[, names(df) != col.name], dummy[, 2:ncol(dummy)])
  return(df)
}
@

\title{Mathematics Developers Survey 2016}
\author{Nejc Ilenic}
\date{}
\maketitle

\section{Introduction}
Anonymised responses from Stack Overflow Annual Developer Survey are published each year along the results to encourage their further analysis. Being curious about where in the world and in which domain a data scientist should start his / her career, we attempt to answer some of the relevant questions by analysing the available dataset.

\vspace{2mm}

An important thing to note when interpreting the results however is that this data may not be a represantative sample from the population of mathematics developers. One should keep in mind that these are developers who were aware of the survey and were willing to answer the questions.

\section{Data preparation}
The dataset was constructed from survey that took place from January 7 to January 25, 2016, with responses originating from Stack Overflow, Stack Exchange technical sites, Facebook and Twitter. Most of the questions are demographic or relating to professional work and technology. Raw data consists of 56030 samples and 66 features, all of which are optional.

In order to obtain an adequately sizable sample, we have decided to include all respondents that belong to the occupation group of mathematics developers, which includes data scientists, machine learning developers and developers with statistics and mathematics backgrounds. After filtering out other occupations and responses with unknown countries there are 2132 samples left for the analysis.

\section{Exploratory analysis}
We are primarily interested in answering two questions: where in the world and in which domains (industries) are mathematics developers satisfied with their jobs the most. Additionally, we want to learn how job satisfaction depends on other factors like compensation, age, gender, etc. We will attempt to answer the first two questions by comparing the level of satisfaction across groups and the last one by estimating linear relationships between variables.

\subsection{Job satisfaction among countries}
Number of mathematics developers per countries can be seen in Figure \ref{fig_0}. Minimum number of 35 respondents is required to take the country into account and all others are placed into a single group called \textit{Other}. Note that selected countries and number of answers may be different when doing inference of specific features due to missing values (i.e. optional answers in the survey). Majority of respondents are from United States, followed by a combination of countries with less than 40 developers, United Kingdom, Germany and India.

<<echo=FALSE>>=
# NUMBER OF RESPONDENTS PER COUNTRY
num.respondents.plot = ggplot(GetNonNaDataCountry('country')) +
  geom_bar(aes(x = reorder(country, country, function(x) - length(x))), fill = '#ff5900') +
  geom_text(stat = 'count', aes(x = country, label = ..count..), vjust = -.05) +
  labs(x = 'country', y = 'number of respondents') +
  theme(axis.text.x = element_text(
    size  = 10,
    angle = 45,
    hjust = 1,
    vjust = 1)
  )

ggsave(
  paste('../plots/num_respondents_per_country.png'),
  num.respondents.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
num.respondents.plot
@
\caption{Number of mathematics developers per country.}\label{fig_0}
\end{figure}

The question from the survey regarding job satisfaction is: 'How satisfied are you with your current job(s)?' with six possible answers: 'I don't have a job', 'I hate my job', 'I'm somewhat dissatisfied with my job', 'I'm neither satisfied nor dissatisfied', 'I'm somewhat satisfied with my job' and 'I love my job'. This is a typical approach to scaling answers in survey research and we will treat the response as a categorical variable for group comparison. Additionally, we will discard answers of people without the job, resulting in five categories.

A multinomial-dirichlet model can be used to model categorical data and since we are mainly interested in a single category ('I love my job') an alternative would be to transform it to a binary variable and use a binomial-beta model instead. Nevertheless, we will model all five categories simultaneously and focus on a single category when interpreting the results. Note that both dirichlet and beta priors are conjugate for the two sampling models and posteriors could easily be derived analytically, we will still use MCMC approximations as an alternative.

A posterior parameters sample is obtained for each country independently by running a MCMC algorithm for 5000 iterations with 400 warmup samples. We have no opinions or information from related studies regarding parameters, thus uninformative priors are used. Traceplots and MCMC summaries are not included in the report, but can be found in the \textit{plots} and in the \textit{mcmc\_summaries} directories. Nothing abnormal can be spotted and it seems that all the chains have converged.

In Figure \ref{fig_1} a posterior predictive check can be seen. Sampling distributions of log odds of the answer 'I love my job' calculated from posterior predictive samples of the same sizes as the observed samples are plotted for each of the countries and there are no noticeable discrepancies between the replicated the observed data (with respect to the selected statistic). Note that visual posterior predictive checks are 'sanity checks' more than anything else (e.g. their usage as a model selection technique would be quite inadequate).

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: data preparation
params = c('theta[1]', 'theta[2]', 'theta[3]', 'theta[4]', 'theta[5]')
params.labels = c(
  'I hate my job',
  'I\'m somewhat dissatisfied with my job',
  'I\'m neither satisfied nor dissatisfied',
  'I\'m somewhat satisfied with my job',
  'I love my job'
)
params.satisfied = 5
params.unsatisfied = c(1, 2, 3, 4)

data.satisfaction = GetNonNaDataCountry('job_satisfaction')
data.satisfaction$country = droplevels(data.satisfaction$country)
data.satisfaction = data.satisfaction[data.satisfaction$job_satisfaction != 'I don\'t have a job',]
data.satisfaction$job_satisfaction = droplevels(data.satisfaction$job_satisfaction)

reordered.levels = levels(data.satisfaction$job_satisfaction)[c(1, 4, 3, 5, 2)]
data.satisfaction$job_satisfaction = factor(data.satisfaction$job_satisfaction, reordered.levels)
@

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: stan data preparation
countries = levels(data.satisfaction$country)
stan.data = list()

for (country in countries) {
  y = as.numeric(data.satisfaction[data.satisfaction$country == country, 'job_satisfaction'])
  stan.data[[country]] = list(
    n = length(y),
    k = length(params),
    y = y,
    a = c(1, 1, 1, 1, 1)
  )
}
@

<<computation,results=hide,echo=FALSE>>=
# JOB SATISFACTION COMPARISON: model fitting and diagnostic plots saving
stan.fits = list()

for (country in countries) {
  stan.fits[[country]] = stan(
    file = '../models/multinomial_dirichlet.stan',
    data = stan.data[[country]],
    iter = 5000,
    warmup = 400,
    chains = 1
  )

  trace.plot = traceplot(stan.fits[[country]], pars = params)
  ggsave(
    paste('../plots/job_satisfaction_comparison_traceplot_', sub(' ', '_', tolower(country)), '.png', sep = ''),
    trace.plot
  )

  summ = summary(stan.fits[[country]], pars = params)$summary
  write.table(
    summ,
    paste('../mcmc_summaries/job_satisfaction_comparison_summary_', sub(' ', '_', tolower(country)), '.txt', sep = '')
  )
}
@

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: posterior predictive check
empirical.logodds = data.frame()
posterior.logodds = data.frame()

for (country in countries) {
  observed.dataset = stan.data[[country]]$y
  n = stan.data[[country]]$n
  pred.datasets = extract(stan.fits[[country]], pars = 'y_rep')$y_rep

  # empirical log odds
  num.satisfied = length(Filter(function(x) x == params.satisfied, observed.dataset))
  num.unsatisfied = length(Filter(function(x) x %in% params.unsatisfied, observed.dataset))
  df = data.frame(lo = log(num.satisfied / num.unsatisfied), n = n, country = country)
  empirical.logodds = rbind(empirical.logodds, df)

  # replicated log odds
  num.satisfied = pred.datasets[, params.satisfied]
  num.unsatisfied = rowSums(pred.datasets[, params.unsatisfied])
  df = data.frame(lo = log(num.satisfied / num.unsatisfied), country = country)
  posterior.logodds = rbind(posterior.logodds, df)
}

posterior.check.plot = ggplot(posterior.logodds, aes(x = lo)) +
  geom_density(fill = 'dodgerblue') +
  geom_vline(data = empirical.logodds, aes(xintercept = lo), colour = 'red') +
  facet_wrap(~country) +
  labs(x = 'log odds', y = 'density') +
  theme(strip.text = element_text(size=7))

ggsave(
  paste('../plots/job_satisfaction_countries_comparison_posterior_check.png'),
  posterior.check.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.check.plot
@
\caption{Job satisfaction between countries posterior predictive check. Densities are sampling distributions of log odds of the answer 'I love my job' calculated from posterior predictive samples of the same sizes as the observed samples. Red lines indicate the observed log odds.}\label{fig_1}
\end{figure}

Result of the sampling can be seen in Figure \ref{fig_2}. Plotted are 90\% confidence intervals for posterior probabilities of the answer 'I love my job' for all countries. It can be concluded that there are either no meaningful differences among the countries regarding the probability of someone loving his / her job or we simply can't answer the question with this data. We can however take a more indirect approach by estimating which variables are positively correlated with job satisfaction and compare those among groups instead.

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: posterior parameters
posterior.params = data.frame()

for (country in countries) {
  summ = summary(stan.fits[[country]], pars = params[5], probs = c(0.05, 0.95))$summary
  df = data.frame(
    mean = summ[, c('mean')],
    ci.low = summ[, c('5%')],
    ci.up = summ[, c('95%')]
  )
  df$country = country
  posterior.params = rbind(posterior.params, df)
}

posterior.params.plot = ggplot(posterior.params) +
  geom_boxplot(
    aes(
      x = country,
      middle = mean,
      lower = ci.low,
      upper = ci.up,
      ymin = mean,
      ymax = mean,
      fill = country
    ),
    stat = 'identity'
  ) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'country', y = 'probability of \'I love my job\'')

ggsave(
  paste('../plots/job_satisfaction_countries_comparison_posterior_parameters.png'),
  posterior.params.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.params.plot
@
\caption{90\% confidence intervals for posterior probabilities of the answer 'I love my job' for all countries.}\label{fig_2}
\end{figure}

\subsection{Explanatory variables for job satisfaction}
Here we are concerned with how job satisfaction varies with a set of selected variables. Specifically, we are interested in how person's age, gender, purchasing power, whether he or she works remotely, values unit testing, commits code at least once a day and whether he or she works in a big company and has PhD explain the level of their job satisfaction. Age and purchasing power are continuous and all other variables are binary. It should be stated that purchasing power is calculated as compensation in dollars divided by a Big Mac index of the respondent's country (an informal way of measuring the purchasing power parity, i.e. how many Big Macs can a person buy per year) and company is regarded big if number of employees is more than 99. All other variables should be self explanatory.

We will treat the outcome (5 possible answers as before) as an ordinal variable (i.e. a categorical for which values are ordered, but the distances between them are unknown) to preserve information regarding the order. Ordinal logistic regression with uninformative priors is our model of choice here. Again 5000 posterior samples are drawn using a MCMC algorithm with 400 warmup iterations. Traceplots and MCMC summaries are not included in the report, but can be found in the \textit{plots} and in the \textit{mcmc\_summaries} directories. Nothing abnormal can be spotted and it seems that all the chains have converged.

In Figure \ref{fig_3} a posterior predictive check can be seen. Plotted are histograms of 20 replicated samples along with a histogram of the observed sample. In some plots differences can be spotted between replicated and observed data and in others histograms are almost identical. We will conclude that our model fits the data sufficiently for our purposes and assume that discrepancies are due to the sampling variability.

<<echo=FALSE>>=
# JOB SATISFACTION EFFECTS: data preparation
cols = c(
  'age_midpoint', 'gender', 'salary_midpoint', 'remote', 'job_satisfaction',
  'big_mac_index', 'unit_testing', 'commit_frequency', 'company_size_range',
  'education'
)
relationships.data = na.omit(data.cleaned[, cols])

relationships.data$purchasing_power = relationships.data$salary_midpoint / relationships.data$big_mac_index
relationships.data = relationships.data[, !(names(relationships.data) %in% c('salary_midpoint', 'big_mac_index'))]

# job_satisfaction
# 'I hate my job',
# 'I\'m somewhat dissatisfied with my job',
# 'I\'m neither satisfied nor dissatisfied',
# 'I\'m somewhat satisfied with my job',
# 'I love my job'
relationships.data = relationships.data[relationships.data$job_satisfaction != 'I don\'t have a job',]
relationships.data$job_satisfaction = droplevels(relationships.data$job_satisfaction)
reordered.levels = levels(relationships.data$job_satisfaction)[c(1, 4, 3, 5, 2)]
relationships.data$job_satisfaction = factor(relationships.data$job_satisfaction, reordered.levels)
relationships.data$job_satisfaction = as.numeric(relationships.data$job_satisfaction)

# gender
# 'Female'
# 'Male'
relationships.data$gender = as.numeric(relationships.data$gender)
relationships.data$gender_male = relationships.data$gender - 1
relationships.data = relationships.data[, names(relationships.data) != 'gender']

# unit_testing
# 'No'
# 'Yes'
relationships.data = relationships.data[relationships.data$unit_testing != 'I don\'t know',]
relationships.data$unit_testing = droplevels(relationships.data$unit_testing)
relationships.data$unit_testing = as.numeric(relationships.data$unit_testing)
relationships.data$unit_testing = relationships.data$unit_testing - 1

# remote
# 'Never'
# 'I rarely work remotely'
# 'Part-time remote'
# 'Full-time remote'
reordered.levels = levels(relationships.data$remote)[c(3, 2, 4, 1)]
relationships.data$remote = factor(relationships.data$remote, reordered.levels)
relationships.data$remote = as.numeric(relationships.data$remote)
# 'No'
# 'Yes'
relationships.data$remote = as.numeric(relationships.data$remote > 2)

# commit_frequency
# 'I never check-in or commit code'
# 'A few times a month'
# 'A couple times a week'
# 'Once a day'
# 'Multiple times a day'
omit = 'I don\'t \"check-in or commit code\", but I do put code into production somewhat frequently'
relationships.data = relationships.data[relationships.data$commit_frequency != omit,]
relationships.data$commit_frequency = droplevels(relationships.data$commit_frequency)
reordered.levels = levels(relationships.data$commit_frequency)[c(3, 2, 1, 5, 4)]
relationships.data$commit_frequency = factor(relationships.data$commit_frequency, reordered.levels)
relationships.data$commit_frequency = as.numeric(relationships.data$commit_frequency)
# commit at least once a day
# 'No'
# 'Yes'
relationships.data$commit_once_a_day = as.numeric(relationships.data$commit_frequency > 3)
relationships.data = relationships.data[, names(relationships.data) != 'commit_frequency']

# company_size_range
relationships.data = relationships.data[relationships.data$company_size_range != 'I am not sure',]
relationships.data$company_size_range = droplevels(relationships.data$company_size_range)
reordered.levels = levels(relationships.data$company_size_range)[c(10, 1, 7, 3, 6, 5, 9, 2, 8, 4)]
relationships.data$company_size_range = factor(relationships.data$company_size_range, reordered.levels)
relationships.data$company_size_range = as.numeric(relationships.data$company_size_range)
# big company
# 'No'
# 'Yes'
relationships.data$big_company = as.numeric(relationships.data$company_size_range > 5)
relationships.data = relationships.data[, names(relationships.data) != 'company_size_range']

# phd
relationships.data$phd = as.numeric(Map(function(x) grepl('PhD', x), relationships.data$education))
relationships.data = relationships.data[, names(relationships.data) != 'education']
@

<<echo=FALSE>>=
# JOB SATISFACTION EFFECTS: stan data preparation
x = relationships.data[, names(relationships.data) != 'job_satisfaction']

# center, scale
x$purchasing_power = (x$purchasing_power - mean(x$purchasing_power)) / sd(x$purchasing_power)
x$age_midpoint = (x$age_midpoint - mean(x$age_midpoint)) / sd(x$age_midpoint)

y = relationships.data$job_satisfaction

k = length(unique(y))
stan.data = list(
  n = nrow(x),
  k = k,
  d = ncol(x),
  y = y,
  x = x
)
@

<<computation,results=hide,echo=FALSE>>=
# JOB SATISFACTION EFFECTS: model fitting and diagnostic plots saving
params = c(
  paste('beta[', 1:ncol(x), ']', sep = ''),
  paste('threshold[', seq(1, k - 1, 1), ']', sep = '')
)

stan.fit = stan(
  file = '../models/ordinal_logistic_regression.stan',
  data = stan.data,
  iter = 5000,
  warmup = 400,
  chains = 1
)

trace.plot = traceplot(stan.fit, pars = params)
ggsave('../plots/job_satisfaction_effects_traceplot.png', trace.plot)

summ = summary(stan.fit, pars = params)$summary
write.table(summ, file = '../mcmc_summaries/job_satisfaction_effects_summary.txt')
@

<<echo=FALSE>>=
# JOB SATISFACTION EFFECTS: posterior predictive check
num.plots = 20
pred.datasets = t(tail(as.data.frame(stan.fit, pars = 'y_rep'), n = num.plots))
pred.datasets = melt(pred.datasets)[, 2:3]

y_rep = data.frame(category = pred.datasets[, 2], replication = pred.datasets[, 1], sample = 'replicated')
y_obs = data.frame(category = rep(y, num.plots), replication = pred.datasets[, 1], sample = 'observed')

predictive.check = rbind(y_rep, y_obs)

predictive.check.plot = ggplot(predictive.check) +
  geom_bar(aes(x = category, fill = sample), position = 'dodge') +
  scale_fill_manual(values = c('dodgerblue', 'red')) +
  facet_wrap(~replication, nrow = 5, ncol = 4) +
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    legend.position = 'bottom'
  ) +
  labs(x = 'job satisfaction category', y = 'count')

ggsave('../plots/job_satisfaction_effects_posterior_check.png')
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=5,echo=FALSE>>=
predictive.check.plot
@
\caption{Posterior predictive check for conditional distribution of job satisfaction given explanatory variables. Plotted are histograms of 20 replicated samples along with a histogram of the observed sample.}\label{fig_3}
\end{figure}

The observed Pearson correlation coefficients (covariance of the two variables divided by the product of their standard deviations) of selected explanatory variables are plotted in Figure \ref{fig_4}. There are no strong correlations between regressors with exception of correlation between age and purchasing power ($+0.36$), which we have to keep in mind when interpreting the regression coefficients. Otherwise these results seem somewhat reasonable: remote work is positively correlated with age and negatively with working in a big company and age is positively correlated with purchasing power.

<<echo=FALSE>>=
# JOB SATISFACTION EFFECTS: regressors correlation
names(x)[names(x) == 'age_midpoint'] = 'age'
names(x)[names(x) == 'remote'] = 'works_remote'
names(x)[names(x) == 'unit_testing'] = 'values_unit_testing'
names(x)[names(x) == 'gender_male'] = 'is_male'
names(x)[names(x) == 'commit_once_a_day'] = 'commits_once_a_day'
names(x)[names(x) == 'big_company'] = 'works_in_big_company'
names(x)[names(x) == 'phd'] = 'has_phd'

corr = round(cor(x), 2)

corr.plot = ggcorrplot(
  corr,
  lab = TRUE,
  lab_size = 2.5,
  outline.col = "white",
  colors = c('#E46726', 'white', 'olivedrab3'),
  legend.title = 'Correlation',
  tl.cex = 8
)

ggsave('../plots/job_satisfaction_effects_regressors_correlation.png', corr.plot)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
corr.plot
@
\caption{Pearson correlation coefficients of selected explanatory variables.}\label{fig_4}
\end{figure}

Next we examine posterior regression coefficients' 90\% confidence intervals in Figure \ref{fig_5}. Let's first focus on continuous variables age and purchasing power. We can't be certain but it seems that as age grows satisfaction level declines. On the other hand we can say much more confidently that purchasing power is positively correlated with job satisfaction, which seems quite reasonable. With dummy variables we have to keep in mind that we have modeled a 'reference' female developer which doesn't commit code at least once a day, doesn't have a PhD, doesn't value unit testing, doesn't work in a big company and doesn't work remotely. We can then intepret signs of coefficients as positive or negative effect that corresponding variables have if we change their values from false to true keeping all other variables constant. For example if we say that our 'reference' developer starts to work remotely it is possible that her satisfaction level will rise, although we should be conservative here as the 5th percentile is actually below zero. Similarly if she starts to work in a big company it is likely that her satisfaction level will decline. 90\% confidence intervals of other variables contain the value of zero so we won't conclude their effect on response variable. Our level of uncertainty is the lowest for purchasing power so we turn our attention to that in the next section.

<<echo=FALSE>>=
# JOB SATISFACTION EFFECTS: posterior parameters
params = params[1:ncol(x)]
regressors = names(x)
posterior.params = data.frame()

for (i in seq_along(regressors)) {
  summ = summary(stan.fit, pars = params[i], probs = c(0.05, 0.95))$summary
  df = data.frame(
    mean = summ[, c('mean')],
    ci.low = summ[, c('5%')],
    ci.up = summ[, c('95%')]
  )
  df$regressor = regressors[i]
  posterior.params = rbind(posterior.params, df)
}

posterior.params.plot = ggplot(posterior.params) +
  geom_boxplot(
    aes(
      x = regressor,
      middle = mean,
      lower = ci.low,
      upper = ci.up,
      ymin = mean,
      ymax = mean,
      fill = regressor
    ),
    stat = 'identity'
  ) +
  geom_hline(aes(yintercept = 0), colour = 'red', size = 1.2) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'explanatory variable', y = 'effect')

ggsave('../plots/job_satisfaction_effects_posterior_parameters.png', posterior.params.plot)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.params.plot
@
\caption{90\% confidence intervals for posterior regression coefficients of selected explanatory variables for job satisfaction.}\label{fig_5}
\end{figure}

\subsection{Purchasing power among countries}
We will compare purchasing power among countries as results in previous section strongly suggest its positive correlation with job satisfaction. In spite of that one should bear in mind that the highest purchasing power doesn't imply the highest job satisfaction and that the opposite case can occur just as well. There may be other (even unmeasured) factors that we have not taken into account and have stronger (negative) correlation with job satisfaction. The only conclusions we will be able to draw based on results from this section are regarding the purchasing power itself.

In Figure \ref{fig_6} we can see the observed densities of purchasing power for all countries. Based on the plots it seems that gamma or Weibull models would be appropriate for this random variable, however as we are only interested in comparing means across groups we can use the normal model (central limit theorem). Additionally we turn to the hierarchical normal model to combine the information from all countries.

<<echo=False>>=
# PURCHASING POWER COMPARISON: data preparation, observed plots
data.salary = GetNonNaDataCountry('salary_midpoint', 'big_mac_index')
data.salary$country = droplevels(data.salary$country)
data.salary$purchasing_power = data.salary$salary_midpoint / data.salary$big_mac_index

purchasing.power.observed.density.plot = ggplot(data.salary) +
  geom_density(aes(x = purchasing_power, fill = country)) +
  facet_wrap(~country) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    ),
    strip.text = element_text(size=7)
  ) +
  labs(x = 'purchasing power', y = 'density')

purchasing.power.observed.box.plot = ggplot(data.salary) +
  geom_boxplot(aes(x = country, y = purchasing_power, fill = country)) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'country', y = 'purchasing power')

ggsave(
  paste('../plots/purchasing_power_countries_observed_density.png'),
  purchasing.power.observed.density.plot
)
ggsave(
  paste('../plots/purchasing_power_countries_observed_box_plot.png'),
  purchasing.power.observed.box.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
purchasing.power.observed.density.plot
@
\caption{Observed densities of purchasing power for all countries.}\label{fig_6}
\end{figure}

A posterior sample of group means is obtained for each country independently by running a MCMC algorithm for 5000 iterations with 400 warmup samples. Like before we have no opinions or information from related studies regarding parameters, thus uninformative priors are used. Traceplots and MCMC summaries are not included in the report, but can be found in the \textit{plots} and in the \textit{mcmc\_summaries} directories. Nothing abnormal can be spotted and it seems that all the chains have converged.

Posterior predictive check can be seen in Figure \ref{fig_7}. Sampling distributions of means of posterior predictive samples of the same sizes as the observed samples are plotted along with the observed means. We can observe a practical demonstration of the central limit theorem (larger sample size - smaller standard error). There are no noticeable discrepancies between the replicated the observed data with respect to the selected statistic (the mean).

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: stan data preparation
countries = levels(data.salary$country)

k = length(countries)
n = c()
ys = list()

for (country in countries) {
  country.purchasing_power = data.salary$purchasing_power[data.salary$country == country]
  n = c(n, length(country.purchasing_power))
  ys[[country]] = country.purchasing_power
}

max_n = max(n)
y = data.frame(matrix(NA, nrow = max_n, ncol = k))
colnames(y) = countries

for (country in countries) {
  length(ys[[country]]) = max_n
  y[, country] = ys[[country]]
}

# stan doesn't support NA
y[is.na(y)] = 0

stan.data = list(
  k = k,
  n = n,
  max_n = max_n,
  y = y
)
@

<<computation,results=hide,echo=FALSE>>=
# PURCHASING POWER COMPARISON: model fitting and diagnostic plots saving
params = c(
  'bg_mu', 'bg_s2', 'wg_s2',
  paste('wg_mu[', 1:k, ']', sep = '')
)

stan.fit = stan(
  file = '../models/hierarchical_normal_inv_gamma.stan',
  data = stan.data,
  iter = 5000,
  warmup = 400,
  chains = 1
)

trace.plot = traceplot(stan.fit, pars = params)
ggsave('../plots/purchasing_power_countries_traceplot.png', trace.plot)

summ = summary(stan.fit, pars = params)$summary
write.table(summ, file = '../mcmc_summaries/purchasing_power_countries_summary.txt')
@

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: posterior predictive check
mu.obs = data.frame()
for (i in seq_along(countries)) {
  y.obs = y[1:n[i], countries[i]]
  mu.obs = rbind(mu.obs, data.frame(country = countries[i], mean = mean(y.obs)))
}

pred.datasets = as.data.frame(stan.fit, pars = 'mu_rep')
names(pred.datasets) = countries

mu.rep = melt(pred.datasets)
names(mu.rep) = c('country', 'mean')

posterior.check.plot = ggplot(mu.rep, aes(x = mean)) +
  geom_density(fill = 'dodgerblue') +
  geom_vline(data = mu.obs, aes(xintercept = mean), colour = 'red') +
  facet_wrap(~country) +
  labs(x = 'mean', y = 'density') +
  theme(
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    ),
    strip.text = element_text(size=7)
  )

ggsave(
  paste('../plots/purchasing_power_countries_posterior_check.png'),
  posterior.check.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.check.plot
@
\caption{Purchasing power between countries posterior predictive check. Densities are sampling distributions of means of posterior predictive samples of the same sizes as the observed samples. Red lines indicate the observed means.}\label{fig_7}
\end{figure}

Result of the sampling can be seen in Figure \ref{fig_8}. Plotted are 90\% confidence intervals for purchasing power posterior means for all countries. We won't calculate exact probabilities of comparisons of means (although we could, we have the posterior distributions), but rather compare them visually. Mean purchasing power is higher in United States and Australia than in all other countries (more than 20000 Big Macs per year). United Kingdom has higher mean purchasing power than all other countries (excluding United States and Australia) and seems similar to Switzerland's, although level of uncertainty is much higher for the latter due to a smaller sample size. Canada, Germany and all countries with less than 35 respondents combined have higher mean purchasing power than Italy.

In next sections the same comparisons for job satisfaction and purchasing power are given for different domains (industries) rather than countries. All methodologies (models, number of iterations of a MCMC algorithm, etc) are identical to previous sections, thus only interpretations are given.

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: posterior parameters
params = params[4:length(params)]
posterior.params = data.frame()

for (i in seq_along(countries)) {
  summ = summary(stan.fit, pars = params[i], probs = c(0.05, 0.95))$summary
  df = data.frame(
    mean = summ[, c('mean')],
    ci.low = summ[, c('5%')],
    ci.up = summ[, c('95%')]
  )
  df$country = countries[i]
  posterior.params = rbind(posterior.params, df)
}

posterior.params.plot = ggplot(posterior.params) +
  geom_boxplot(
    aes(
      x = country,
      middle = mean,
      lower = ci.low,
      upper = ci.up,
      ymin = mean,
      ymax = mean,
      fill = country
    ),
    stat = 'identity'
  ) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'country', y = 'purchasing power')

ggsave('../plots/purchasing_power_countries_posterior_parameters.png', posterior.params.plot)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.params.plot
@
\caption{90\% confidence intervals for purchasing power posterior means for all countries.}\label{fig_8}
\end{figure}

\subsection{Job satisfaction among industries}
Number of mathematics developers per industries can be seen in Figure \ref{fig_9}. As before, minimum number of 35 respondents is required to take the industry into account and all others are placed into a single group called \textit{Other}. Note that selected industries and number of answers may be different when doing inference of specific features due to missing values (i.e. optional answers in the survey).

<<echo=FALSE>>=
# NUMBER OF RESPONDENTS PER INDUSTRY
num.respondents.plot = ggplot(GetNonNaDataIndustry('industry')) +
  geom_bar(aes(x = reorder(industry, industry, function(x) - length(x))), fill = '#ff5900') +
  geom_text(stat = 'count', aes(x = industry, label = ..count..), vjust = -.05) +
  labs(x = 'industry', y = 'number of respondents') +
  theme(axis.text.x = element_text(
    size  = 8,
    angle = 45,
    hjust = 1,
    vjust = 1)
  )

ggsave(
  paste('../plots/num_respondents_per_industry.png'),
  num.respondents.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
num.respondents.plot
@
\caption{Number of mathematics developers per industry.}\label{fig_9}
\end{figure}

In Figure \ref{fig_10} a posterior predictive check can be seen. Like previously, sampling distributions of log odds of the answer 'I love my job' calculated from posterior predictive samples of the same sizes as the observed samples are plotted for each of the industries and there are no noticeable discrepancies between the replicated the observed data (with respect to the selected statistic). An interesting regard is that the log odds ratio, like the mean, also has a normal asymptotic distribution.

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: data preparation
params = c('theta[1]', 'theta[2]', 'theta[3]', 'theta[4]', 'theta[5]')
params.labels = c(
  'I hate my job',
  'I\'m somewhat dissatisfied with my job',
  'I\'m neither satisfied nor dissatisfied',
  'I\'m somewhat satisfied with my job',
  'I love my job'
)
params.satisfied = 5
params.unsatisfied = c(1, 2, 3, 4)

data.satisfaction = GetNonNaDataIndustry('job_satisfaction')
data.satisfaction$industry = droplevels(data.satisfaction$industry)
data.satisfaction = data.satisfaction[data.satisfaction$job_satisfaction != 'I don\'t have a job',]
data.satisfaction$job_satisfaction = droplevels(data.satisfaction$job_satisfaction)

reordered.levels = levels(data.satisfaction$job_satisfaction)[c(1, 4, 3, 5, 2)]
data.satisfaction$job_satisfaction = factor(data.satisfaction$job_satisfaction, reordered.levels)
@

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: stan data preparation
industries = levels(data.satisfaction$industry)
stan.data = list()

for (industry in industries) {
  y = as.numeric(data.satisfaction[data.satisfaction$industry == industry, 'job_satisfaction'])
  stan.data[[industry]] = list(
    n = length(y),
    k = length(params),
    y = y,
    a = c(1, 1, 1, 1, 1)
  )
}
@

<<computation,results=hide,echo=FALSE>>=
# JOB SATISFACTION COMPARISON: model fitting and diagnostic plots saving
stan.fits = list()

for (industry in industries) {
  stan.fits[[industry]] = stan(
    file = '../models/multinomial_dirichlet.stan',
    data = stan.data[[industry]],
    iter = 5000,
    warmup = 400,
    chains = 1
  )

  trace.plot = traceplot(stan.fits[[industry]], pars = params)
  file.name = sub('/', '', sub(' ', '_', tolower(industry)))
  ggsave(
    paste('../plots/job_satisfaction_comparison_traceplot_', file.name, '.png', sep = ''),
    trace.plot
  )

  summ = summary(stan.fits[[industry]], pars = params)$summary
  write.table(
    summ,
    paste('../mcmc_summaries/job_satisfaction_comparison_summary_', file.name, '.txt', sep = '')
  )
}
@

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: posterior predictive check
empirical.logodds = data.frame()
posterior.logodds = data.frame()

for (industry in industries) {
  observed.dataset = stan.data[[industry]]$y
  n = stan.data[[industry]]$n
  pred.datasets = extract(stan.fits[[industry]], pars = 'y_rep')$y_rep

  # empirical log odds
  num.satisfied = length(Filter(function(x) x == params.satisfied, observed.dataset))
  num.unsatisfied = length(Filter(function(x) x %in% params.unsatisfied, observed.dataset))
  df = data.frame(lo = log(num.satisfied / num.unsatisfied), n = n, industry = industry)
  empirical.logodds = rbind(empirical.logodds, df)

  # replicated log odds
  num.satisfied = pred.datasets[, params.satisfied]
  num.unsatisfied = rowSums(pred.datasets[, params.unsatisfied])
  df = data.frame(lo = log(num.satisfied / num.unsatisfied), industry = industry)
  posterior.logodds = rbind(posterior.logodds, df)
}

posterior.check.plot = ggplot(posterior.logodds, aes(x = lo)) +
  geom_density(fill = 'dodgerblue') +
  geom_vline(data = empirical.logodds, aes(xintercept = lo), colour = 'red') +
  facet_wrap(~industry) +
  labs(x = 'log odds', y = 'density') +
  theme(strip.text = element_text(size=7))

ggsave(
  paste('../plots/job_satisfaction_industries_comparison_posterior_check.png'),
  posterior.check.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.check.plot
@
\caption{Job satisfaction between industries posterior predictive check. Densities are sampling distributions of log odds of the answer 'I love my job' calculated from posterior predictive samples of the same sizes as the observed samples. Red lines indicate the observed log odds.}\label{fig_10}
\end{figure}

Result of the sampling can be seen in Figure \ref{fig_11}. Plotted are 90\% confidence intervals for posterior probabilities of the answer 'I love my job' for all industries. The only speculations we can make with this data is that for the Software Products and Education domains the probability of the answer 'I love my job' seems higher than for the Finance / Banking domain (although we will stay conservative and not make any conclusions).

<<echo=FALSE>>=
# JOB SATISFACTION COMPARISON: posterior parameters
posterior.params = data.frame()

for (industry in industries) {
  summ = summary(stan.fits[[industry]], pars = params[5], probs = c(0.05, 0.95))$summary
  df = data.frame(
    mean = summ[, c('mean')],
    ci.low = summ[, c('5%')],
    ci.up = summ[, c('95%')]
  )
  df$industry = industry
  posterior.params = rbind(posterior.params, df)
}

posterior.params.plot = ggplot(posterior.params) +
  geom_boxplot(
    aes(
      x = industry,
      middle = mean,
      lower = ci.low,
      upper = ci.up,
      ymin = mean,
      ymax = mean,
      fill = industry
    ),
    stat = 'identity'
  ) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'industry', y = 'probability of \'I love my job\'')

ggsave(
  paste('../plots/job_satisfaction_industries_comparison_posterior_parameters.png'),
  posterior.params.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.params.plot
@
\caption{90\% confidence intervals for posterior probabilities of the answer 'I love my job' for all industries.}\label{fig_11}
\end{figure}

\subsection{Purchasing power among industries}
In Figure \ref{fig_12} the observed densities of purchasing power for all industries can be seen. The gamma or Weibull distributions could again be used as our first attempts at modeling if we wouldn't only be interested in the mean.

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: data preparation, observed plots
data.salary = GetNonNaDataIndustry('salary_midpoint', 'big_mac_index')
data.salary$industry = droplevels(data.salary$industry)
data.salary$purchasing_power = data.salary$salary_midpoint / data.salary$big_mac_index

purchasing.power.observed.density.plot = ggplot(data.salary) +
  geom_density(aes(x = purchasing_power, fill = industry)) +
  facet_wrap(~industry) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    ),
    strip.text = element_text(size=7)
  ) +
  labs(x = 'purchasing power', y = 'density')

purchasing.power.observed.box.plot = ggplot(data.salary) +
  geom_boxplot(aes(x = industry, y = purchasing_power, fill = industry)) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'industry', y = 'purchasing power')

ggsave(
  paste('../plots/purchasing_power_industries_observed_density.png'),
  purchasing.power.observed.density.plot
)
ggsave(
  paste('../plots/purchasing_power_industries_observed_box_plot.png'),
  purchasing.power.observed.box.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
purchasing.power.observed.density.plot
@
\caption{Observed densities of purchasing power for all industries.}\label{fig_12}
\end{figure}

Posterior predictive check can be seen in Figure \ref{fig_13}. Sampling distributions of means of posterior predictive samples of the same sizes as the observed samples are plotted along with the observed means. There are no noticeable discrepancies between the replicated the observed data with respect to the selected statistic (the mean).

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: stan data preparation
industries = levels(data.salary$industry)

k = length(industries)
n = c()
ys = list()

for (industry in industries) {
  industry.purchasing_power = data.salary$purchasing_power[data.salary$industry == industry]
  n = c(n, length(industry.purchasing_power))
  ys[[industry]] = industry.purchasing_power
}

max_n = max(n)
y = data.frame(matrix(NA, nrow = max_n, ncol = k))
colnames(y) = industries

for (industry in industries) {
  length(ys[[industry]]) = max_n
  y[, industry] = ys[[industry]]
}

# stan doesn't support NA
y[is.na(y)] = 0

stan.data = list(
  k = k,
  n = n,
  max_n = max_n,
  y = y
)
@

<<computation,results=hide,echo=FALSE>>=
# PURCHASING POWER COMPARISON: model fitting and diagnostic plots saving
params = c(
  'bg_mu', 'bg_s2', 'wg_s2',
  paste('wg_mu[', 1:k, ']', sep = '')
)

stan.fit = stan(
  file = '../models/hierarchical_normal_inv_gamma.stan',
  data = stan.data,
  iter = 5000,
  warmup = 400,
  chains = 1
)

trace.plot = traceplot(stan.fit, pars = params)
ggsave('../plots/purchasing_power_industries_traceplot.png', trace.plot)

summ = summary(stan.fit, pars = params)$summary
write.table(summ, file = '../mcmc_summaries/purchasing_power_industries_summary.txt')
@

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: posterior predictive check
mu.obs = data.frame()
for (i in seq_along(industries)) {
  y.obs = data.salary$purchasing_power[data.salary$industry == industries[i]]
  mu.obs = rbind(mu.obs, data.frame(industry = industries[i], mean = mean(y.obs)))
}

pred.datasets = as.data.frame(stan.fit, pars = 'mu_rep')
names(pred.datasets) = industries

mu.rep = melt(pred.datasets)
names(mu.rep) = c('industry', 'mean')
levels(mu.rep$industry) = levels(data.salary$industry)

posterior.check.plot = ggplot(mu.rep, aes(x = mean)) +
  geom_density(fill = 'dodgerblue') +
  geom_vline(data = mu.obs, aes(xintercept = mean), colour = 'red') +
  facet_wrap(~industry) +
  labs(x = 'mean', y = 'density') +
  theme(
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    ),
    strip.text = element_text(size=7)
  )

ggsave(
  paste('../plots/purchasing_power_industries_posterior_check.png'),
  posterior.check.plot
)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.check.plot
@
\caption{Purchasing power between industries posterior predictive check. Densities are sampling distributions of means of posterior predictive samples of the same sizes as the observed samples. Red lines indicate the observed means.}\label{fig_13}
\end{figure}

Result of the sampling can be seen in Figure \ref{fig_14}. Plotted are 90\% confidence intervals for purchasing power posterior means for all industries. We are most certain that the mean purchasing power is higher in Finance / Banking than in all other domains (except in the Internet domain and the Media / Advertising domain). Mathematics developers in Education have lower mean purchasing power than those in the Software Products, Media / Advertising, Internet, Healthcare and Finance / Banking domains.

<<echo=FALSE>>=
# PURCHASING POWER COMPARISON: posterior parameters
params = params[4:length(params)]
posterior.params = data.frame()

for (i in seq_along(industries)) {
  summ = summary(stan.fit, pars = params[i], probs = c(0.05, 0.95))$summary
  df = data.frame(
    mean = summ[, c('mean')],
    ci.low = summ[, c('5%')],
    ci.up = summ[, c('95%')]
  )
  df$industry = industries[i]
  posterior.params = rbind(posterior.params, df)
}

posterior.params.plot = ggplot(posterior.params) +
  geom_boxplot(
    aes(
      x = industry,
      middle = mean,
      lower = ci.low,
      upper = ci.up,
      ymin = mean,
      ymax = mean,
      fill = industry
    ),
    stat = 'identity'
  ) +
  theme(
    legend.position = 'none',
    axis.text.x = element_text(
      angle = 45,
      hjust = 1,
      vjust = 1
    )
  ) +
  labs(x = 'country', y = 'purchasing power')

ggsave('../plots/purchasing_power_industries_posterior_parameters.png', posterior.params.plot)
@

\begin{figure}[H]
\centering
<<fig=True,width=5,height=4,echo=FALSE>>=
posterior.params.plot
@
\caption{90\% confidence intervals for purchasing power posterior means for all industries.}\label{fig_14}
\end{figure}

\section{Conclusion}
In this project we compared probabilities that a mathematics developer loves his / her job and mean purchasing powers among countries and industries. Additionally, we were interested in how job satisfaction varies with a set of selected variables.

We suspect that age and working in a big company is negatively correlated with job satisfaction, we suspect that working remotely is positively correlated with job satisfaction and we are quite certain that purchasing power is positively correlated with job satisfaction. Our degree of belief is quite high that mean purchasing power is the highest in Australia and United States. We also believe, that mean purchasing power is the highest in the Finance / Banking industry.

\end{document}
